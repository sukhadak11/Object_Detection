{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukhadak11/Object_Detection/blob/main/Object_detection_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMCC3um-F4JK",
        "outputId": "cecac455-2f55-4365-b8f9-dbcbc9e2058d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow opencv-python lxml bs4 scikit-learn\n",
        "\n",
        "# Import required packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g0nvaZSJvGx",
        "outputId": "e5ba1ab3-4152-400c-db5c-dce96234e4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNPxd1oPJPvq"
      },
      "outputs": [],
      "source": [
        "# Paths to your dataset\n",
        "train_images_dir = '/content/drive/MyDrive/database/database/train/images'\n",
        "train_annotations_dir = '/content/drive/MyDrive/database/database/train/annotations'\n",
        "valid_images_dir = '/content/drive/MyDrive/database/database/valid/images'\n",
        "valid_annotations_dir = '/content/drive/MyDrive/database/database/valid/annotations'\n",
        "test_images_dir = '/content/drive/MyDrive/database/database/Test/Images'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts_imTYae3FM"
      },
      "source": [
        "its option 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDL6QUEbe5sq"
      },
      "outputs": [],
      "source": [
        "# Step 1: Parse Pascal VOC Annotations\n",
        "def parse_voc_annotation(annotation_file):\n",
        "    tree = ET.parse(annotation_file)\n",
        "    root = tree.getroot()\n",
        "    objects = []\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text.strip()\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "        objects.append({'name': name, 'bbox': [xmin, ymin, xmax, ymax]})\n",
        "    return objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pdjLQ_ie9-F"
      },
      "outputs": [],
      "source": [
        "# Step 2: Check all unique labels in dataset\n",
        "def check_all_labels(annotation_paths):\n",
        "    all_labels = set()\n",
        "    for annotation_path in annotation_paths:\n",
        "        objects = parse_voc_annotation(annotation_path)\n",
        "        for obj in objects:\n",
        "            all_labels.add(obj['name'])\n",
        "    return all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WreRcyk9fBuU"
      },
      "outputs": [],
      "source": [
        "# Step 3: Load Image and Annotation Paths\n",
        "def load_image_annotation_paths(image_dir, annotation_dir):\n",
        "    image_paths, annotation_paths = [], []\n",
        "    for image_file in os.listdir(image_dir):\n",
        "        if image_file.endswith(('.jpg', '.png')):\n",
        "            image_paths.append(os.path.join(image_dir, image_file))\n",
        "            annotation_file = os.path.join(annotation_dir, image_file.replace('.jpg', '.xml').replace('.png', '.xml'))\n",
        "            annotation_paths.append(annotation_file)\n",
        "    return image_paths, annotation_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iA8-gi7fH1y"
      },
      "outputs": [],
      "source": [
        "# Step 4: Custom Data Generator using Keras Sequence\n",
        "class ObjectDetectionDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, annotation_paths, label_encoder, batch_size=8, img_size=(224, 224)):\n",
        "        self.image_paths = image_paths\n",
        "        self.annotation_paths = annotation_paths\n",
        "        self.label_encoder = label_encoder\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = len(label_encoder.classes_)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_image_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_annotation_paths = self.annotation_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "        batch_bboxes = []\n",
        "\n",
        "        for img_path, annot_path in zip(batch_image_paths, batch_annotation_paths):\n",
        "            # Load and preprocess image\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), self.img_size)\n",
        "            image = image / 255.0\n",
        "\n",
        "            # Parse annotations\n",
        "            objects = parse_voc_annotation(annot_path)\n",
        "\n",
        "            if objects:  # Only process if there are objects in the image\n",
        "                obj = objects[0]  # For simplicity, we'll only use the first object\n",
        "                class_id = self.label_encoder.transform([obj['name']])[0]\n",
        "                bbox = obj['bbox']\n",
        "\n",
        "                # Normalize bbox coordinates\n",
        "                x_min, y_min, x_max, y_max = bbox\n",
        "                x_min, x_max = x_min / self.img_size[1], x_max / self.img_size[1]\n",
        "                y_min, y_max = y_min / self.img_size[0], y_max / self.img_size[0]\n",
        "\n",
        "                batch_images.append(image)\n",
        "                batch_labels.append(class_id)\n",
        "                batch_bboxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        return np.array(batch_images), {'class_output': np.array(batch_labels), 'bbox_output': np.array(batch_bboxes)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95DYpV41fMNP"
      },
      "outputs": [],
      "source": [
        "# Step 5: Initialize Label Encoder\n",
        "def initialize_label_encoder(train_annotation_paths, valid_annotation_paths):\n",
        "    train_labels = check_all_labels(train_annotation_paths)\n",
        "    valid_labels = check_all_labels(valid_annotation_paths)\n",
        "    classes = list(train_labels.union(valid_labels))\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(classes)\n",
        "    return label_encoder, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIjPvl0cfPgT"
      },
      "outputs": [],
      "source": [
        "# Step 6: Define the Object Detection Model\n",
        "def create_model(input_shape, num_classes):\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "    class_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='class_output')(x)\n",
        "    bbox_output = tf.keras.layers.Dense(4, activation='sigmoid', name='bbox_output')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=[class_output, bbox_output])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyFavoHVfTN7",
        "outputId": "592c286b-13ba-473e-e844-9963aaef9f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Custom Loss Function\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "def bbox_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.MSE(y_true, y_pred) # Use MSE instead of mean_squared_error.\n",
        "                                               # Both are valid and equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlwzF5qWfXJU"
      },
      "outputs": [],
      "source": [
        "# Step 8: Train the Model\n",
        "def train_model(train_generator, valid_generator, input_shape, num_classes, epochs=10):\n",
        "    model = create_model(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'class_output': 'sparse_categorical_crossentropy',\n",
        "                        'bbox_output': bbox_loss},\n",
        "                  loss_weights={'class_output': 1.0, 'bbox_output': 1.0},\n",
        "                  metrics={'class_output': 'accuracy'})\n",
        "    model.fit(train_generator, validation_data=valid_generator, epochs=epochs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm5tYBd-fa1J"
      },
      "outputs": [],
      "source": [
        "# Step 9: Save the Trained Model\n",
        "def save_model(model, filename='object_detection_model.h5'):\n",
        "    model.save(filename)\n",
        "    print(f\"Model saved as '{filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvRq5X9tfgYd"
      },
      "outputs": [],
      "source": [
        "# Step 10: Perform Object Detection on Video\n",
        "def perform_video_object_detection(model, label_encoder, video_path, output_path, img_size=(224, 224), confidence_threshold=0.5):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), 20, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Preprocess the frame\n",
        "        img_resized = cv2.resize(frame, img_size)\n",
        "        img_normalized = img_resized / 255.0\n",
        "        img_expanded = np.expand_dims(img_normalized, axis=0)\n",
        "\n",
        "        # Perform prediction\n",
        "        class_pred, bbox_pred = model.predict(img_expanded)\n",
        "\n",
        "        # Process predictions\n",
        "        class_id = np.argmax(class_pred[0])\n",
        "        class_prob = np.max(class_pred[0])\n",
        "        if class_prob > confidence_threshold:\n",
        "            class_label = label_encoder.inverse_transform([class_id])[0]\n",
        "            x_min, y_min, x_max, y_max = bbox_pred[0]\n",
        "\n",
        "            # Denormalize bbox coordinates\n",
        "            x_min, x_max = int(x_min * frame_width), int(x_max * frame_width)\n",
        "            y_min, y_max = int(y_min * frame_height), int(y_max * frame_height)\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, f\"{class_label}: {class_prob:.2f}\", (x_min, y_min - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Save the frame with the prediction\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved at {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5LsksYglrHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9a520c-4831-45e9-d6ec-d9077bb85046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - class_output_accuracy: 0.1169 - loss: 5.5476"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 6s/step - class_output_accuracy: 0.1363 - loss: 5.4665 - val_class_output_accuracy: 0.5000 - val_loss: 3.6713\n",
            "Epoch 2/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8s/step - class_output_accuracy: 0.7326 - loss: 2.0221 - val_class_output_accuracy: 0.4444 - val_loss: 4.0201\n",
            "Epoch 3/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4s/step - class_output_accuracy: 0.8891 - loss: 1.5910 - val_class_output_accuracy: 0.4444 - val_loss: 4.8585\n",
            "Epoch 4/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5s/step - class_output_accuracy: 0.9448 - loss: 1.3268 - val_class_output_accuracy: 0.4444 - val_loss: 5.6151\n",
            "Epoch 5/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5s/step - class_output_accuracy: 0.9783 - loss: 1.1507 - val_class_output_accuracy: 0.3333 - val_loss: 6.2542\n",
            "Epoch 6/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - class_output_accuracy: 0.9790 - loss: 1.0971 - val_class_output_accuracy: 0.3333 - val_loss: 6.8545\n",
            "Epoch 7/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6s/step - class_output_accuracy: 0.9813 - loss: 1.0690 - val_class_output_accuracy: 0.3333 - val_loss: 7.3348\n",
            "Epoch 8/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5s/step - class_output_accuracy: 1.0000 - loss: 1.0530 - val_class_output_accuracy: 0.2778 - val_loss: 7.7843\n",
            "Epoch 9/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5s/step - class_output_accuracy: 0.9813 - loss: 1.0613 - val_class_output_accuracy: 0.2778 - val_loss: 8.1855\n",
            "Epoch 10/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - class_output_accuracy: 0.9580 - loss: 1.0652 - val_class_output_accuracy: 0.2778 - val_loss: 8.5185\n",
            "Epoch 11/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5s/step - class_output_accuracy: 0.9790 - loss: 1.0746 - val_class_output_accuracy: 0.2778 - val_loss: 8.7689\n",
            "Epoch 12/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - class_output_accuracy: 1.0000 - loss: 1.0178 - val_class_output_accuracy: 0.2778 - val_loss: 8.9609\n",
            "Epoch 13/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5s/step - class_output_accuracy: 0.9813 - loss: 1.0290 - val_class_output_accuracy: 0.2778 - val_loss: 9.1050\n",
            "Epoch 14/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8s/step - class_output_accuracy: 1.0000 - loss: 1.0946 - val_class_output_accuracy: 0.2778 - val_loss: 9.2131\n",
            "Epoch 15/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8s/step - class_output_accuracy: 1.0000 - loss: 1.1115 - val_class_output_accuracy: 0.2778 - val_loss: 9.2762\n",
            "Epoch 16/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5s/step - class_output_accuracy: 1.0000 - loss: 0.9959 - val_class_output_accuracy: 0.2778 - val_loss: 9.2994\n",
            "Epoch 17/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8s/step - class_output_accuracy: 1.0000 - loss: 1.1098 - val_class_output_accuracy: 0.2778 - val_loss: 9.2839\n",
            "Epoch 18/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - class_output_accuracy: 1.0000 - loss: 0.9885 - val_class_output_accuracy: 0.2778 - val_loss: 9.2711\n",
            "Epoch 19/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8s/step - class_output_accuracy: 1.0000 - loss: 1.1090 - val_class_output_accuracy: 0.2778 - val_loss: 9.2573\n",
            "Epoch 20/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5s/step - class_output_accuracy: 1.0000 - loss: 1.0144 - val_class_output_accuracy: 0.2778 - val_loss: 9.2393\n",
            "Processed video saved at /content/drive/MyDrive/Video_output\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "import xml.etree.ElementTree as ET\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up paths\n",
        "    train_images_dir = '/content/drive/MyDrive/database/database/train/images'\n",
        "    train_annotations_dir = '/content/drive/MyDrive/database/database/train/annotations'\n",
        "    valid_images_dir = '/content/drive/MyDrive/database/database/valid/images'\n",
        "    valid_annotations_dir = '/content/drive/MyDrive/database/database/valid/annotations'\n",
        "\n",
        "    # Load paths\n",
        "    train_image_paths, train_annotation_paths = load_image_annotation_paths(train_images_dir, train_annotations_dir)\n",
        "    valid_image_paths, valid_annotation_paths = load_image_annotation_paths(valid_images_dir, valid_annotations_dir)\n",
        "\n",
        "    # Initialize label encoder\n",
        "    label_encoder, classes = initialize_label_encoder(train_annotation_paths, valid_annotation_paths)\n",
        "\n",
        "    # Create data generators\n",
        "    batch_size = 32\n",
        "    img_size = (224, 224)\n",
        "    train_generator = ObjectDetectionDataGenerator(train_image_paths, train_annotation_paths, label_encoder, batch_size, img_size)\n",
        "    valid_generator = ObjectDetectionDataGenerator(valid_image_paths, valid_annotation_paths, label_encoder, batch_size, img_size)\n",
        "\n",
        "    # Train the model\n",
        "    input_shape = (*img_size, 3)\n",
        "    num_classes = len(classes)\n",
        "    model = train_model(train_generator, valid_generator, input_shape, num_classes, epochs=20)\n",
        "\n",
        "\n",
        "    # Perform object detection on a video\n",
        "    video_path = '/content/drive/MyDrive/video'\n",
        "    output_path = '/content/drive/MyDrive/Video_output'\n",
        "    perform_video_object_detection(model, label_encoder, video_path, output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BWSfYw9rbdX8VTrJPiv1F4Xpqsdxd9YD",
      "authorship_tag": "ABX9TyNmdlw4WaGE5ILOHGbQcg1G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}